import sys
import os
import json
import logging
from typing import TypedDict, List, Dict
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from datetime import datetime

# âœ… Fix Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)

# âœ… Import retriever
try:
    from agents.retriever_agent import retrieve_chunks
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)

# âœ… Load environment variables
load_dotenv()

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# âœ… Define pipeline state
class QAState(TypedDict):
    raw_query: str
    retrieved_chunks: List[Dict]
    final_answer: str
    timestamp: str

# âœ… STEP 1: Retrieve chunks
def retrieve_node(state: QAState) -> QAState:
    query = state["raw_query"]
    logger.info(f"ğŸ” Retrieving chunks for: {query}")

    try:
        chunks = retrieve_chunks(query, k=12)
        logger.info(f"âœ… Retrieved {len(chunks)} chunks for query: {query}")
        return {"retrieved_chunks": chunks or []}
    except Exception as e:
        logger.error(f"âŒ Retrieval error: {e}")
        return {"retrieved_chunks": []}

# âœ… STEP 2: Generate answer using GPT (with improved filtering and fallback)
def answer_node(state: QAState) -> QAState:
    query = state["raw_query"]
    chunks = state.get("retrieved_chunks", [])

    if not chunks:
        logger.warning(f"âš  No chunks found for: {query}")
        return {"final_answer": "Sorry, I couldnâ€™t find this in the policy database. Please contact Bajaj Allianz for details."}

    # âœ… Filter and rank high-scoring chunks
    high_quality_chunks = [c for c in chunks if c.get("score", 0) >= 0.85]
    high_quality_chunks = sorted(high_quality_chunks, key=lambda x: x["score"], reverse=True)
    top_chunks = high_quality_chunks[:6] if high_quality_chunks else chunks[:3]

    # âœ… Combine chunks into one context
    context_text = "\n\n".join([c["text"] for c in top_chunks])

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
    prompt = f"""
    You are an expert Bajaj Allianz Health Insurance assistant.

    User Question: {query}

    Context from the policy:
    {context_text}

    ğŸ”€ Write a clear, concise, and accurate answer using only the above policy context.
    ğŸ”€ If context partially answers the question, summarize what is available.
    ğŸ”€ If there is no relevant content, respond with:
    \"This information is not available in the policy database.\"
    """

    try:
        response = llm.invoke(prompt)
        answer = response.content.strip()

        if "not available" in answer.lower():
            fallback_text = top_chunks[0]["text"]
            logger.info("âš  GPT fallback triggered â€” returning top chunk from retriever")
            answer = f"From the policy: {fallback_text.strip().splitlines()[0]}..."

        logger.info(f"âœ… Answer generated for query: {query}")
        return {"final_answer": answer}
    except Exception as e:
        logger.error(f"âŒ GPT answer generation failed: {e}")
        return {"final_answer": "Failed to generate an answer at this time."}

# âœ… STEP 3: Add timestamp
def finalize_node(state: QAState) -> QAState:
    state["timestamp"] = datetime.now().isoformat()
    return state

# âœ… Build the graph
graph = StateGraph(QAState)
graph.add_node("retrieve", retrieve_node)
graph.add_node("answer", answer_node)
graph.add_node("finalize", finalize_node)

graph.set_entry_point("retrieve")
graph.add_edge("retrieve", "answer")
graph.add_edge("answer", "finalize")
graph.add_edge("finalize", END)

try:
    qa_app = graph.compile()
    logger.info("âœ… FAQ/QA Graph compiled successfully")
except Exception as e:
    logger.error(f"âŒ Graph compilation failed: {e}")
    sys.exit(1)

# âœ… Function to run the pipeline
def run_faq_pipeline(query: str) -> Dict:
    logger.info(f"ğŸš€ Running FAQ pipeline for: {query}")
    try:
        result = qa_app.invoke({"raw_query": query})
        return {
            "query": query,
            "answers": [result.get("final_answer", "")],
            "chunks_used": result.get("retrieved_chunks", []),
            "timestamp": result.get("timestamp", "")
        }
    except Exception as e:
        logger.error(f"âŒ FAQ pipeline failed: {e}")
        return {
            "query": query,
            "answers": ["Pipeline failed to process query."],
            "chunks_used": [],
            "timestamp": datetime.now().isoformat()
        }

# âœ… Self-test
if __name__ == "__main__":
    print("\n=== ğŸ§ª FAQ PIPELINE SELF-TEST ===\n")
    test_query = "What is covered under domiciliary hospitalization and what are its conditions?"
    result = run_faq_pipeline(test_query)

    print(f"ğŸ” Query: {result['query']}")
    print(f"âœ… Answer: {result['answers'][0]}\n")
    print("ğŸ“¦ Chunks Used:")
    for idx, c in enumerate(result["chunks_used"], 1):
        print(f"{idx}. (Score: {c.get('score', 0):.2f}) {c['text'][:150]}...")